from nltk.corpus import stopwords
from nltk.tokenize import wordpunct_tokenize, sent_tokenize, word_tokenize
import nltk
import sklearn.datasets
import sklearn.metrics
import sklearn.model_selection
from sklearn.svm import SVC
nltk.download('averaged_perceptron_tagger')
import nltk
nltk.download('tagsets')
from nltk.data import load
tagdict = load('help/tagsets/upenn_tagset.pickle')
# tagdict.keys()
import numpy as np
# Download the 'stopwords' and 'punkt' from the Natural Language Toolkit, you can comment the next lines if already present.
nltk.download('stopwords')
nltk.download('punkt')
stop_words = set(stopwords.words('english'))
# nltk.help.upenn_tagset()

def load_data(dir_name):
    return sklearn.datasets.load_files('data/%s' % dir_name, encoding='utf-8')


def load_train_data():
    return load_data('train')


def load_test_data():
    return load_data('test')

from sklearn.feature_extraction.text import CountVectorizer

train=load_train_data()
test=load_test_data()
from sklearn.feature_extraction.text import CountVectorizer

def get_most_common_words(limit):
    vectorizer = CountVectorizer()
    data=load_train_data().data
    corpus = data
    vectorizer = CountVectorizer(stop_words='english', analyzer='word', token_pattern=r'\b\w\w+\b')
    term_matrix = vectorizer.fit_transform(corpus).todense()    
    wordcounts=(np.sum(term_matrix, axis=0)).reshape(18278,1).tolist()
    count_all=[]
    for c in wordcounts:
        count_all.append(c[0])
    uniquewords=vectorizer.get_feature_names()
    worddict={}
    for feature,c in zip(uniquewords, count_all):
        worddict[feature]=c
    sort_dict_word=sorted(worddict.items(), key=lambda item: item[1], reverse=True)
    sort_dict_word=sort_dict_word[:limit]
    top_freq_word=[]
    for s in sort_dict_word:
        top_freq_word.append(s[0])       
    return top_freq_word


import re
    
  #Bag-of-words
def get_word_vec(limit, text):
    punctuations = "!()-[]{};:'\,<>./""?@#$%^&*_~“”"
    no_punct = ""
    for char in text:
       if char not in punctuations:
           no_punct = no_punct + char
    no_punct=no_punct.lower()
    word_list=no_punct.split()
    word_dictt={}
    top_freq_word=get_most_common_words(limit)
    for w in word_list:
        for frew in top_freq_word:
            if w==frew:
                word_dictt[w]=word_dictt.get(w,0)+1
            else:
                word_dictt[frew]=word_dictt.get(frew,0)
    word_vec=[]
    for fre in word_dictt.values():
        word_vec.append(fre)
    maxval=max(word_vec)
    minval=min(word_vec)
    norm=[]
    for i in word_vec:
        normvals=(i-minval)/(maxval-minval)
        norm.append(normvals)
    return norm

# len_feature_wordvec=len(get_word_vec(200, text))

    # TODO: Follow the instructions in the assignment and add your own features.

# from nltk import ngrams
#Function words
def funcword_freq(text):
    text=re.sub('’', "'", text)
    text=re.sub('”', "'", text)
    text=re.sub('“', "'", text)
    word_tokenized= word_tokenize(text)
    pos_tag=nltk.pos_tag(word_tokenized)
    all_pos=[]
    for p in pos_tag:
        all_pos+=[p[1]]
    function_words=['DT', 'IN', 'MD', 'PRP', 'PRP$', 'WDT', 'WP','CC', 'VBP', 'VBZ', 'EX', 'FW', 'PDT', 'POS', 'RBR', 'RBS', 'UH']
    funcword_dict={}
    for fw in function_words:
        for p in all_pos:
            if p == fw:
                funcword_dict[p]=funcword_dict.get(p,0)+1
            else:
                funcword_dict[fw]=funcword_dict.get(fw,0)
    funcword_dict=sorted(funcword_dict.items())     
    funcword_freq=[]
    for i in funcword_dict:
        funcword_freq.append(i[1])
    maxval=max(funcword_freq)
    minval=min(funcword_freq)
    norm=[]
    for i in funcword_freq:
        normvals=(i-minval)/(maxval-minval)
        norm.append(normvals)

    return norm

# POS tag frequencies
def pos_freq(text):
    text=re.sub('’', "'", text)
    text=re.sub('”', "'", text)
    text=re.sub('“', "'", text)
    word_tokenized= word_tokenize(text)
    pos_tag=nltk.pos_tag(word_tokenized)
    puncs=",.:;""''!@#$%^&*()-+=_/?[]{}|><“”’~``"
    all_pos=[]
    for p in pos_tag:
        all_pos+=[p[1]]
    pos_set=['LS', 'TO', 'VBN', "''", 'WP', 'UH', 'VBG', 'JJ', 'VBZ', '--', 'VBP', 'NN', 'DT', 'PRP', ':', 'WP$', 'NNPS', 'PRP$', 'WDT', '(', ')', '.', ',', '``', '$', 'RB', 'RBR', 'RBS', 'VBD', 'IN', 'FW', 'RP', 'JJR', 'JJS', 'PDT', 'MD', 'VB', 'WRB', 'NNP', 'EX', 'NNS', 'SYM', 'CC', 'CD', 'POS']
    pos_dict={}
    for p in all_pos:
        for pos in pos_set:
            if p == pos:
                pos_dict[p]=pos_dict.get(p,0)+1
            else:
                pos_dict[pos]=pos_dict.get(pos,0)
    pos_dict=sorted(pos_dict.items()) 
    # print(pos_dict)    
    pos_freq=[]
    for i in pos_dict:
        pos_freq.append(i[1])
    maxval=max(pos_freq)
    minval=min(pos_freq)
    norm=[]
    for i in pos_freq:
        normvals=(i-minval)/(maxval-minval)
        norm.append(normvals)

    return norm

#Punctuation frequencies

def punc_freq(text):

    tok=word_tokenize(text)
    punc=",.:;""''!@#$%^&*()-+=_/?[]{}|><“”``~`"
    punc_dict={}
    for p in punc:
        for c in tok:
            if c ==p:
                punc_dict[c]=punc_dict.get(c,0)+1
            else:
                punc_dict[p]=punc_dict.get(p,0)
    punc_dict=sorted(punc_dict.items())     
    punc_freq=[]
    for p in punc_dict:
        punc_freq.append(p[1])
    maxval=max(punc_freq)
    minval=min(punc_freq)
    norm=[]
    for i in punc_freq:
        normvals=(i-minval)/(maxval-minval)
        norm.append(normvals)
    return norm

#Typetoken
def type_token(text):
    punctuations = "!()-[]{};:'\,<>./""?@#$%^&*_~“”"
    no_punct = ""
    for char in text:
       if char not in punctuations:
           no_punct = no_punct + char
    no_punct=no_punct.lower()
    word_list=no_punct.split()
    word_list_no_stopwords=[w for w in word_list if w not in stop_words]
    unique_words=set(word_list_no_stopwords)
    
    type_token=len(unique_words)/len(word_list)
    return type_token

len_feature_typetoken=1

#Get all features
def extract_features(text):  
    features=[]
    features.extend(punc_freq(text))
    features.extend(funcword_freq(text))
    features.append(type_token(text))
    features.extend(pos_freq(text))
    features.extend(get_word_vec(200,text))
    features=np.array(features)
    return features


#Punctuation frequencies left out
def extract_features_no_punc_freq(text):  
    features=[]
    features.extend(funcword_freq(text))
    features.append(type_token(text))
    features.extend(pos_freq(text))
    features.extend(get_word_vec(200,text))
    features=np.array(features)
    return features
#Function words left out
def extract_features_no_funcword_freq(text):  
    features=[]
    features.extend(punc_freq(text))
    features.append(type_token(text))
    features.extend(pos_freq(text))
    features.extend(get_word_vec(200,text))
    return features
#Typetoken left out
def extract_features_no_typetoken(text):  
    features=[]
    features.extend(punc_freq(text))
    features.extend(funcword_freq(text))
    features.extend(pos_freq(text))
    features.extend(get_word_vec(200,text))
    features=np.array(features)
    return features
#POS tags left out
def extract_features_no_posfreq(text):  
    features=[]
    features.extend(punc_freq(text))
    features.extend(funcword_freq(text))
    features.append(type_token(text))
    features.extend(get_word_vec(200,text))
    features=np.array(features)
    return features

#Bag-of-words left out
def extract_features_no_wordvec(text):  
    features=[]
    features.extend(punc_freq(text))
    features.extend(funcword_freq(text))
    features.append(type_token(text))
    features.extend(pos_freq(text))
    features=np.array(features)
    return features

def extract_wordvec(text):  
    features=[]
    features.extend(get_word_vec(200,text))
    features=np.array(features)
    return features
train_data=train.data 
train_labels=train.target
test_data=test.data
test_label=test.target

# Classify using the features
def classify(train_features, train_labels, test_features):
    # TODO: (Optional) If you would like to test different how classifiers would perform different, you can alter
    # TODO: the classifier here.
    clf = SVC(kernel='linear')
    clf.fit(train_features, train_labels)
    return clf.predict(test_features)
import numpy as np
# classify=classify(all_features_train, train_labels, all_features_test)

# Evaluate predictions (y_pred) given the ground truth (y_true)
def evaluate(y_true, y_pred):
    # TODO: What is being evaluated here and what does it say about the performance? Include or change the evaluation
    # TODO: if necessary.
    recall = sklearn.metrics.recall_score(y_true, y_pred, average='macro')
    print("Recall: %f" % recall)

    precision = sklearn.metrics.precision_score(y_true, y_pred, average='macro')
    print("Precision: %f" % precision)

    f1_score = sklearn.metrics.f1_score(y_true, y_pred, average='macro')
    print("F1-score: %f" % f1_score)

    return recall, precision, f1_score


def main():
    train_data = load_train_data()
    
    # Extract the features
    features = list(map(extract_features, train_data.data))
    
    # Classify and evaluate
    skf = sklearn.model_selection.StratifiedKFold(n_splits=10)
    scores = []
    for fold_id, (train_indexes, validation_indexes) in enumerate(skf.split(train_data.filenames, train_data.target)):
        # Print the fold number
        print("Fold %d" % (fold_id + 1))
    
        # Collect the data for this train/validation split
        train_features = [features[x] for x in train_indexes]
        train_labels = [train_data.target[x] for x in train_indexes]
        validation_features = [features[x] for x in validation_indexes]
        validation_labels = [train_data.target[x] for x in validation_indexes]
    
        # Classify and add the scores to be able to average later
        y_pred = classify(np.array(train_features), np.array(train_labels), np.array(validation_features))
        print(type(y_pred))
        scores.append(evaluate(np.array(validation_labels), y_pred))
    
        # Print a newline
        print("")
    
    # Print the averaged score
    recall = sum([x[0] for x in scores]) / len(scores)
    # print("Averaged total recall", recall)
    precision = sum([x[1] for x in scores]) / len(scores)
    # print("Averaged total precision", precision)
    f_score = sum([x[2] for x in scores]) / len(scores)
    # print("Averaged total f-score", f_score)
    # print("")
    return recall, precision, f_score


 
features = list(map(extract_features, train.data))
    
test_data = load_test_data()
test_features = list(map(extract_features, test_data.data))

y_pred = classify(np.array(features), train.target, np.array(test_features))
R, P, f1=evaluate(np.array(test_data.target), y_pred)


def main_no_funcword_freq(): 
    train_data = load_train_data()
    
    # Extract the features
    features = list(map(extract_features_no_funcword_freq, train_data.data)) 
    
    # Classify and evaluate
    skf = sklearn.model_selection.StratifiedKFold(n_splits=10)
    scores = []
    for fold_id, (train_indexes, validation_indexes) in enumerate(skf.split(train_data.filenames, train_data.target)):
        # Print the fold number
        print("Fold %d" % (fold_id + 1))
    
        # Collect the data for this train/validation split
        train_features = [features[x] for x in train_indexes]
        train_labels = [train_data.target[x] for x in train_indexes]
        validation_features = [features[x] for x in validation_indexes]
        validation_labels = [train_data.target[x] for x in validation_indexes]
    
        # Classify and add the scores to be able to average later
        y_pred = classify(np.array(train_features), np.array(train_labels), np.array(validation_features))
        print(type(y_pred))
        scores.append(evaluate(np.array(validation_labels), y_pred))
    
        # Print a newline
        print("")
    
    # Print the averaged score
    recall = sum([x[0] for x in scores]) / len(scores)
    # print("Averaged total recall", recall)
    precision = sum([x[1] for x in scores]) / len(scores)
    # print("Averaged total precision", precision)
    f_score = sum([x[2] for x in scores]) / len(scores)
    # print("Averaged total f-score", f_score)
    # print("")
    return recall, precision, f_score


features_nofuncword_freq = list(map(extract_features_no_funcword_freq, train.data)) 
    
test_data = load_test_data()
test_features_nofuncwordfreq= list(map(extract_features_no_funcword_freq, test_data.data))

y_pred_nofucnwordfreq = classify(np.array(features_nofuncword_freq), train.target, np.array(test_features_nofuncwordfreq))
R_nofuncwordfreq, P_nofuncwordfreq, f1_nofuncwordfreq=evaluate(np.array(test_data.target), y_pred_nofucnwordfreq)
#-----------------------------------------------------------------------------------

def main_no_punc_freq(): 
    train_data = load_train_data()
    
    # Extract the features
    features = list(map(extract_features_no_punc_freq, train_data.data)) 
    
    # Classify and evaluate
    skf = sklearn.model_selection.StratifiedKFold(n_splits=10)
    scores = []
    for fold_id, (train_indexes, validation_indexes) in enumerate(skf.split(train_data.filenames, train_data.target)):
        # Print the fold number
        print("Fold %d" % (fold_id + 1))
    
        # Collect the data for this train/validation split
        train_features = [features[x] for x in train_indexes]
        train_labels = [train_data.target[x] for x in train_indexes]
        validation_features = [features[x] for x in validation_indexes]
        validation_labels = [train_data.target[x] for x in validation_indexes]
    
        # Classify and add the scores to be able to average later
        y_pred = classify(np.array(train_features), np.array(train_labels), np.array(validation_features))
        print(type(y_pred))
        scores.append(evaluate(np.array(validation_labels), y_pred))
    
        # Print a newline
        print("")
    
    # Print the averaged score
    recall = sum([x[0] for x in scores]) / len(scores)
    # print("Averaged total recall", recall)
    precision = sum([x[1] for x in scores]) / len(scores)
    # print("Averaged total precision", precision)
    f_score = sum([x[2] for x in scores]) / len(scores)
    # print("Averaged total f-score", f_score)
    # print("")
    return recall, precision, f_score


features_nopunc_freq = list(map(extract_features_no_punc_freq, train.data)) 
    
test_data = load_test_data()
test_features_nopunc_fre= list(map(extract_features_no_punc_freq, test_data.data))

y_pred_nopuncfreq = classify(np.array(features_nopunc_freq), train.target, np.array(test_features_nopunc_fre))
R_nopuncfreq, P_nopuncfreq, f1_nopuncfreq=evaluate(np.array(test_data.target), y_pred_nopuncfreq)
# -------------------------------------------------------------------------------------

def main_no_typetoken(): 
    train_data = load_train_data()
    
    # Extract the features
    features = list(map(extract_features_no_typetoken, train_data.data)) 
    
    # Classify and evaluate
    skf = sklearn.model_selection.StratifiedKFold(n_splits=10)
    scores = []
    for fold_id, (train_indexes, validation_indexes) in enumerate(skf.split(train_data.filenames, train_data.target)):
        # Print the fold number
        print("Fold %d" % (fold_id + 1))
    
        # Collect the data for this train/validation split
        train_features = [features[x] for x in train_indexes]
        train_labels = [train_data.target[x] for x in train_indexes]
        validation_features = [features[x] for x in validation_indexes]
        validation_labels = [train_data.target[x] for x in validation_indexes]
    
        # Classify and add the scores to be able to average later
        y_pred = classify(np.array(train_features), np.array(train_labels), np.array(validation_features))
        print(type(y_pred))
        scores.append(evaluate(np.array(validation_labels), y_pred))
    
        # Print a newline
        print("")
    
    # Print the averaged score
    recall = sum([x[0] for x in scores]) / len(scores)
    # print("Averaged total recall", recall)
    precision = sum([x[1] for x in scores]) / len(scores)
    # print("Averaged total precision", precision)
    f_score = sum([x[2] for x in scores]) / len(scores)
    # print("Averaged total f-score", f_score)
    # print("")
    return recall, precision, f_score



features_notypetoken = list(map(extract_features_no_typetoken, train.data)) 
    
test_data = load_test_data()
test_features_notypetoken= list(map(extract_features_no_typetoken, test_data.data))

y_pred_notypetoken = classify(np.array(features_notypetoken), train.target, np.array(test_features_notypetoken))
R_notypetoken, P_notypetoken, f1_notypetoken=evaluate(np.array(test_data.target), y_pred_notypetoken)
#--------------------------------------------------------------------------------

def main_no_wordvec(): 
    train_data = load_train_data()
    
    # Extract the features
    features = list(map(extract_features_no_wordvec, train_data.data)) 
    
    # Classify and evaluate
    skf = sklearn.model_selection.StratifiedKFold(n_splits=10)
    scores = []
    for fold_id, (train_indexes, validation_indexes) in enumerate(skf.split(train_data.filenames, train_data.target)):
        # Print the fold number
        print("Fold %d" % (fold_id + 1))
    
        # Collect the data for this train/validation split
        train_features = [features[x] for x in train_indexes]
        train_labels = [train_data.target[x] for x in train_indexes]
        validation_features = [features[x] for x in validation_indexes]
        validation_labels = [train_data.target[x] for x in validation_indexes]
    
        # Classify and add the scores to be able to average later
        y_pred = classify(np.array(train_features), np.array(train_labels), np.array(validation_features))
        print(type(y_pred))
        scores.append(evaluate(np.array(validation_labels), y_pred))
    
        # Print a newline
        print("")
    
    # Print the averaged score
    recall = sum([x[0] for x in scores]) / len(scores)
    # print("Averaged total recall", recall)
    precision = sum([x[1] for x in scores]) / len(scores)
    # print("Averaged total precision", precision)
    f_score = sum([x[2] for x in scores]) / len(scores)
    # print("Averaged total f-score", f_score)
    # print("")
    return recall, precision, f_score

features_nowordvec = list(map(extract_features_no_wordvec, train.data)) 
    
test_data = load_test_data()
test_features_nowordvec= list(map(extract_features_no_wordvec, test_data.data))

y_pred_nowordvec = classify(np.array(features_nowordvec), train.target, np.array(test_features_nowordvec)) 
R_nowordvec, P_nowordvec, f1_nowordvec=evaluate(np.array(test_data.target), y_pred_nowordvec)
#---------------------------------------------------------------------
def main_noposfreq():
    train_data = load_train_data()
    
    # Extract the features
    features = list(map(extract_features_no_posfreq, train_data.data)) 
    
    # Classify and evaluate
    skf = sklearn.model_selection.StratifiedKFold(n_splits=10)
    scores = []
    for fold_id, (train_indexes, validation_indexes) in enumerate(skf.split(train_data.filenames, train_data.target)):
        # Print the fold number
        print("Fold %d" % (fold_id + 1))
    
        # Collect the data for this train/validation split
        train_features = [features[x] for x in train_indexes]
        train_labels = [train_data.target[x] for x in train_indexes]
        validation_features = [features[x] for x in validation_indexes]
        validation_labels = [train_data.target[x] for x in validation_indexes]
    
        # Classify and add the scores to be able to average later
        y_pred = classify(np.array(train_features), np.array(train_labels), np.array(validation_features))
        print(type(y_pred))
        scores.append(evaluate(np.array(validation_labels), y_pred))
    
        # Print a newline
        print("")
    
    # Print the averaged score
    recall = sum([x[0] for x in scores]) / len(scores)
    # print("Averaged total recall", recall)
    precision = sum([x[1] for x in scores]) / len(scores)
    # print("Averaged total precision", precision)
    f_score = sum([x[2] for x in scores]) / len(scores)
    # print("Averaged total f-score", f_score)
    # print("")
    return recall, precision, f_score

features_noposfreq = list(map(extract_features_no_posfreq, train.data)) 
    
test_data = load_test_data()
test_features_noposfreq= list(map(extract_features_no_posfreq, test_data.data))

y_pred_noposfreq = classify(np.array(features_noposfreq), train.target, np.array(test_features_noposfreq)) 
R_noposfreq, P_noposfreq, f1_noposfreq=evaluate(np.array(test_data.target), y_pred_noposfreq)


def wordvec():
    train_data = load_train_data()
    
    # Extract the features
    features = list(map(extract_wordvec, train_data.data))
    
    # Classify and evaluate
    skf = sklearn.model_selection.StratifiedKFold(n_splits=10)
    scores = []
    for fold_id, (train_indexes, validation_indexes) in enumerate(skf.split(train_data.filenames, train_data.target)):
        # Print the fold number
        print("Fold %d" % (fold_id + 1))
    
        # Collect the data for this train/validation split
        train_features = [features[x] for x in train_indexes]
        train_labels = [train_data.target[x] for x in train_indexes]
        validation_features = [features[x] for x in validation_indexes]
        validation_labels = [train_data.target[x] for x in validation_indexes]
    
        # Classify and add the scores to be able to average later
        y_pred = classify(np.array(train_features), np.array(train_labels), np.array(validation_features))
        print(type(y_pred))
        scores.append(evaluate(np.array(validation_labels), y_pred))
    
        # Print a newline
        print("")
    
    # Print the averaged score
    recall = sum([x[0] for x in scores]) / len(scores)
    # print("Averaged total recall", recall)
    precision = sum([x[1] for x in scores]) / len(scores)
    # print("Averaged total precision", precision)
    f_score = sum([x[2] for x in scores]) / len(scores)
    # print("Averaged total f-score", f_score)
    # print("")
    return recall, precision, f_score


features_WV = list(map(extract_wordvec, train.data))
    
test_data_WV = load_test_data()
test_features_WV = list(map(extract_wordvec, test_data_WV.data))

y_pred_WV = classify(np.array(features_WV), train.target, np.array(test_features_WV))
R_WV, P_WV, f1_WV=evaluate(np.array(test_data_WV.target), y_pred_WV)
#-------------------------------------------------------
Arecall_noposfreq, Aprecision_noposfreq, Af_score_noposfreq=main_noposfreq()
Arecall_no_wordvec, Aprecision_no_wordvec, Af_score_no_wordvec=main_no_wordvec()
Arecall_no_typetoken, Aprecision_no_typetoken, Af_score_no_typetoken=main_no_typetoken()
ARecall_nopuncfreq, APrecision_nopuncfreq, AFscore_nopuncfreq=main_no_punc_freq()
Arecall_nofuncword_freq, Aprecision_nofuncword_fre, Af_score_nofuncword_fre=main_no_funcword_freq()
Arecall_all, Aprecision_all, AF_score_all=main()
Arecall_WV, Aprecision_WV, Af_score_WV=wordvec()

import matplotlib.pyplot as plt
from matplotlib import pyplot
all_mean_f1=[AF_score_all, Af_score_nofuncword_fre, AFscore_nopuncfreq, Af_score_no_typetoken, Af_score_no_wordvec, Af_score_noposfreq]
#names of feature group left out
names = ['None', 'Function words', 'Punctuation', 'Type-token ratio', 'Bag of words', 'POS tags']
values_mean_f1= all_mean_f1
plt.figure(figsize=(9, 6))
plt.bar(names, values_mean_f1)
plt.show()
#----------------------------------------------------------
all_mean_P=[Aprecision_all, Aprecision_nofuncword_fre, APrecision_nopuncfreq, Aprecision_no_typetoken, Aprecision_no_wordvec, Aprecision_noposfreq]
names = ['None', 'Function words', 'Punctuation', 'Type-token ratio', 'Bag of words', 'POS tags']
values_mean_P= all_mean_P
plt.figure(figsize=(9, 6))
plt.bar(names, values_mean_P)
plt.show()
#------------------------------------------------------

all_mean_R=[R, R_nofuncwordfreq, R_nopuncfreq, R_notypetoken, R_nowordvec, R_noposfreq]
names = ['None', 'Function words', 'Punctuation', 'Type-token ratio', 'Bag of words', 'POS tags']
values_mean_R= all_mean_R
plt.figure(figsize=(9, 6))
plt.bar(names, values_mean_R)
plt.show()

#---------------------------------------------------------------------------
all_f1_test=[f1, f1_nofuncwordfreq, f1_nopuncfreq, f1_notypetoken, f1_nowordvec, f1_noposfreq]
names = ['None', 'Function words', 'Punctuation', 'Type-token ratio', 'Bag of words', 'POS tags']
values_f1_test= all_f1_test
plt.figure(figsize=(9, 6))
plt.bar(names, values_f1_test)
plt.show()

# ---------------------------------------------------------------------------
all_R_test=[R, R_nofuncwordfreq, R_nopuncfreq, R_notypetoken, R_nowordvec, R_noposfreq]
names = ['None', 'Function words', 'Punctuation', 'Type-token ratio', 'Bag of words', 'POS tags']
values_R_test= all_R_test
plt.figure(figsize=(9, 6))
plt.bar(names, values_R_test)
plt.show()
#-----------------------------------------------------------------------------
all_P_test=[P, P_nofuncwordfreq, P_nopuncfreq, P_notypetoken, P_nowordvec, P_noposfreq]
names = ['None', 'Function words', 'Punctuation', 'Type-token ratio', 'Bag of words', 'POS tags']
values_P_test= all_P_test
plt.figure(figsize=(9, 6))
plt.bar(names, values_P_test)
plt.show()

import pandas as pd

#Check misclassification of wordvec
import pandas as pd
df = pd.DataFrame(list(zip(test_data_WV.data, test_data_WV.target, y_pred_WV)), columns=['Text', 'Target', 'Prediction'])  # whole dataset
wrong_preds_df = df.loc[~(df['Target'] == df['Prediction'])]  # extract what the classifier got wrong
s=wrong_preds_df['Prediction']
count_wrong=s.value_counts(normalize=False)
count_wrong=dict(count_wrong)
wrong_preds_df=wrong_preds_df.sort_values(by=['Prediction'])
wrong_preds_df["false positive rate"] = df["Prediction"].apply(lambda x: count_wrong.get(x))

wrong_preds_df=wrong_preds_df.sort_values(by=['false positive rate'], ascending=False)

wrong_preds_df.to_csv('wrong_preds.csv')  # save as .csv file

#Confusion matrix on test set with highest performance features_Wordvec
import sklearn.metrics as metrics
from sklearn.metrics import confusion_matrix
from sklearn.metrics import plot_confusion_matrix
import itertools
import seaborn as sns
import matplotlib.pyplot as plt
CM_WV= confusion_matrix(test_data_WV.target, y_pred_WV)
plt.figure(figsize = (15,15))
sns.heatmap(CM_WV, annot=True, cmap="Blues", fmt = 'g', 
            xticklabels=np.unique(test_label), yticklabels=np.unique(test_label))
plt.xlabel("Predicted Classes")
plt.ylabel("True Classes")
plt.title("Confusion Matrix")
plt.show()






